-------------------------------------------------------------------------------
Using 1137951 tokens for training (100% of 1137951)
-------------------------------------------------------------------------------
Most frequent n-grams for n=1,2,3
(('the',), 63536)             	(('.', '``'), 7753)           	(('.', '``', 'i'), 1452)      
((',',), 47952)               	(('in', 'the'), 6706)         	(('said', '.', '``'), 1149)   
(('.',), 39355)               	(('of', 'the'), 6659)         	((',', "''", 'he'), 1004)     
(('to',), 28752)              	((',', "''"), 4978)           	(('.', '``', 'we'), 850)      
(('of',), 24104)              	(('.', 'the'), 4547)          	(('the', 'united', 'states'), 769)
(('in',), 23446)              	(('.', "''"), 3089)           	((',', "''", 'said'), 767)    
(('and',), 22935)             	((',', 'the'), 2762)          	(("''", 'he', 'said'), 720)   
(('a',), 22643)               	(('to', 'the'), 2723)         	(('.', '``', 'the'), 685)     
(('``',), 12585)              	(('said', '.'), 2373)         	(('he', 'said', '.'), 644)    
(("'s",), 12312)              	(('for', 'the'), 2348)        	(('.', '``', 'it'), 618)      
Saved plot at figure.pdf
-------------------------------------------------------------------------------
Using vocab size 10000 (excluding UNK) (original 41844)
/mnt/c/Users/Seth/Documents/code/util.py:76: RuntimeWarning: divide by zero encountered in log
  logprob += np.log(self.bi_prob[idx, corpus[i+1]])
Train perplexity: 70.967555
Val Perplexity: inf
